\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{geometry}
\geometry{top=20mm, bottom=20mm, left=15mm, right=15mm}
\setlength{\parindent}{0mm}
\setlength{\parskip}{2mm}


\author{Пак Матвей Евгеньевич}
\date{Б05-102}

\begin{document}

\maketitle

\section*{K-means}
\title{Конспект семинара по теме K-means}


\subsection*{Основные понятия}
\textbf{Обучение без учителя:} анализ данных без меток, задача — выявить скрытые структуры.\\
\textbf{Кластеризация:} разделение объектов на группы (кластеры), где объекты внутри группы схожи.\\
\textbf{K-means:}
\begin{itemize}
    \item Делит данные на $K$ кластеров.
    \item Использует евклидово расстояние для расчета близости.
    \item Основные этапы:
    \begin{enumerate}
        \item Инициализация центроидов.
        \item Назначение объектов к ближайшим центроидам.
        \item Пересчет центроидов.
        \item Повторение до сходимости.
    \end{enumerate}
\end{itemize}

\subsection*{Построение данных}
\begin{itemize}
    \item Используется распределение Дирихле (\texttt{np.random.dirichlet}) с добавлением шума (\texttt{np.random.randn}).
    \item Генерируются два набора данных ($X_1$ и $X_2$), объединенные в $X$.
    \item Визуализация данных:
    \[
    \texttt{plt.plot(X[:, 0], X[:, 1], '.', color=colors[0])}
    \]
\end{itemize}

\subsection*{Пример работы K-means}
\textbf{Шаги эксперимента:}
\begin{enumerate}
    \item Инициализация модели:
    \[
    \texttt{model = KMeans(n\_clusters=n, random\_state=42)}
    \]
    \item Обучение модели:
    \[
    \texttt{model.fit(X)}
    \]
    \item Визуализация кластеров:
    \begin{itemize}
        \item Точки кластера:
        \[
        \texttt{plt.plot(X[model.labels\_ == i, 0], X[model.labels\_ == i, 1], '.', color=colors[i])}
        \]
        \item Центры кластеров:
        \[
        \texttt{plt.plot([model.cluster\_centers\_[i][0]], [model.cluster\_centers\_[i][1]], 'x', c=colors[i], markersize=20)}
        \]
    \end{itemize}
\end{enumerate}

\textbf{Результаты:}
\begin{itemize}
    \item Запуск алгоритма с разным числом кластеров ($n\_clusters = 2, 4, 8$).
    \item При увеличении числа кластеров разделение становится более детальным.
\end{itemize}

\subsection*{Итоги и обсуждение}
\textbf{Преимущества K-means:}
\begin{itemize}
    \item Простота и скорость.
    \item Эффективность при небольшом числе кластеров.
\end{itemize}
\textbf{Недостатки K-means:}
\begin{itemize}
    \item Зависимость от начальной инициализации.
    \item Чувствительность к шуму и выбросам.
\end{itemize}
\textbf{Применение:} маркетинг, медицина, анализ поведения пользователей и др.



\subsection*{Задачи}

\textbf{Задача 1:} \textit{Предположим, у вас есть данные:} 
\[
X = \left\{
\begin{array}{cc}
(1, 2), & (2, 3), \\
(6, 7), & (7, 8)
\end{array}
\right\}.
\]
\textit{Используя K-means, разделите эти данные на 2 кластера.}

\textbf{Решение:}  
Алгоритм начнется с случайных центроидов. После первого шага, кластеры могут быть следующими:
- Кластер 1: $(1, 2), (2, 3)$
- Кластер 2: $(6, 7), (7, 8)$

Итоговые центры кластеров:
- Центр кластера 1: $(1.5, 2.5)$
- Центр кластера 2: $(6.5, 7.5)$

---

\textbf{Задача 2:} \textit{Даны три точки: $(1, 1)$, $(2, 2)$ и $(6, 6)$. Используя K-means с $n=2$, определите, как они будут разделены на два кластера.}

\textbf{Решение:}  
- Инициализация центроидов: случайно выбираем $(1, 1)$ и $(6, 6)$.
- Первый шаг:
  - Точка $(1, 1)$ ближе к центроиду $(1, 1)$.
  - Точка $(2, 2)$ ближе к центроиду $(1, 1)$.
  - Точка $(6, 6)$ ближе к центроиду $(6, 6)$.
- После пересчета центроидов:
  - Центр кластера 1: $(1.5, 1.5)$
  - Центр кластера 2: $(6, 6)$
  
Таким образом, точки $(1, 1)$ и $(2, 2)$ будут в одном кластере, а точка $(6, 6)$ в другом.

---

\textbf{Задача 3:} \textit{Предположим, у вас есть 6 точек: $(1, 2)$, $(2, 3)$, $(3, 4)$, $(8, 9)$, $(9, 10)$, $(10, 11)$. Используя K-means с $n=2$, как будет происходить кластеризация?}

\textbf{Решение:}  
1. Инициализация центроидов: предположим, что начальные центры $(1, 2)$ и $(10, 11)$.
2. После первого шага:
   - Точки $(1, 2)$, $(2, 3)$, $(3, 4)$ ближе к центроиду $(1, 2)$.
   - Точки $(8, 9)$, $(9, 10)$, $(10, 11)$ ближе к центроиду $(10, 11)$.
3. Пересчитаем центроиды:
   - Центр кластера 1: $(2, 3)$
   - Центр кластера 2: $(9, 10)$

Таким образом, кластер 1 содержит точки $(1, 2)$, $(2, 3)$, $(3, 4)$, а кластер 2 — точки $(8, 9)$, $(9, 10)$, $(10, 11)$.

\end{document}

